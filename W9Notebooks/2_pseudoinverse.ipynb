{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo-inverse and SVD\n",
    "\n",
    "This notebook provides a short introduction to the Moore-Penrose pseudo-inverse and the singular value decomposition (SVD). This introduction is more of a teaser for your later studies as we will not develop the mathematics of these, but show a few examples.\n",
    "\n",
    "---\n",
    "\n",
    "The following is modified from the Wikipedia article on the [Moore-Penrose inverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse):\n",
    "\n",
    "The Moore-Penrose pseudo-inverse $A^{+}$ of a matrix $A$ is a widely known generalisation of the inverse matrix. A common use of the pseudo-inverse is to compute a \"best fit\" (least squares or linear regression) solution to a system of linear equations.\"\n",
    "\n",
    "---\n",
    "\n",
    "Recall our linear regression problem from last week. We had an $m \\times n$ design matrix $X$ and an $m$ vector of targets $y$. We want to find a vector $w$ of $n$ weights such that:\n",
    "$$\n",
    "X w = y\n",
    "$$\n",
    "\n",
    "In particular, $X$ was a $4220 \\times 10$ matrix, and $y$ was a vector of length $4220$. As you know, the only way for there to exist $w$ that solves this equation is for $y$ to lie in the column space of $X$. ($X w$ is a linear combination of the columns of $X$.) Of course $y$ won't in general be in the column space of $X$ so finding a solution $w$ means finding a least squares solution, the $w$ that minimises the mean squared error (MSE) between $Xw$ and $y$. \n",
    "\n",
    "There is another way to look at solving for $w$ and relates directly to linear algebra. Let $w = X^{+} y$, where is the Moore-Penrose pseudo-inverse of $X$. This gives exactly the same solution as the least squares solution.\n",
    "\n",
    "---\n",
    "\n",
    "The Moore-Penrose pseudo-inverse has many applications besides least squares. The Numpy linear algebra library contains the routine `np.linalg.pinv` that computes the Moore-Penrose pseudo-inverse of a matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Differentiation matrix \n",
    "\n",
    "A classic example of the pseudo-inverse is \"inverting\" differentiation matrices. \n",
    "\n",
    "Let $T: \\mathbb R[x]_{\\le 3} \\to \\mathbb R[x]_{\\le 3}$ be given by $T(f) = df/dx$ in the basis of monomials: $1, x, x^2, x^3$.\n",
    "\n",
    "The matrix $D$ for this linear transformation is\n",
    "\n",
    "$$\n",
    "D = \n",
    "\\begin{pmatrix}\n",
    "0 & 1 & 0 & 0  \\\\\n",
    "0 & 0 & 2 & 0  \\\\\n",
    "0 & 0 & 0 & 3 \\\\\n",
    "0 & 0 & 0 & 0 \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Such a matrix is easily constructed using `np.diag`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0]\n",
      " [0 0 2 0]\n",
      " [0 0 0 3]\n",
      " [0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# vector d is the first super-diagonal of D. \n",
    "d = np.arange(1,4)\n",
    "\n",
    "# from vector d construct matrix D\n",
    "D = np.diag(d,1)\n",
    "\n",
    "# verify that D is correct\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $f = 2+4x^3$ has coordinate vector $v = (2, 0, 0, 4)$ in the monomial basis. Multiplying this vector by the differentiation matrix gives the derivative of $f$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0 12  0]\n"
     ]
    }
   ],
   "source": [
    "v = np.array([2, 0, 0, 4])\n",
    "v_prime = D @ v\n",
    "\n",
    "print(v_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector `v_prime` corresponds to $12 x^2$, which is indeed the derivative of $f$. \n",
    "\n",
    "---\n",
    "\n",
    "Inverting $D$ corresponds to integration. We know that we cannot invert $D$ because integration take us out of this space, e.g. the integral of $x^3$ is not in our space. Related to this is that integration is not unique; there is an undetermined constant of integration. Equivalently, $D$ has a non-trivial kernel. You should know what the kernel of $D$ is.\n",
    "\n",
    "The Moore-Penrose pseudo-inverse exactly solves these issues and allows us to \"invert\" $D$. First we look at the Python and then discuss. `np.linalg.pinv(D)` computes the pseudo-inverse of $D$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.        ]\n",
      " [1.         0.         0.         0.        ]\n",
      " [0.         0.5        0.         0.        ]\n",
      " [0.         0.         0.33333333 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "D_pinv = np.linalg.pinv(D)\n",
    "\n",
    "print(D_pinv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(D_pinv @ D)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that the top row of `D_pinv` is all zeros means that no matter what we multiply `D_pinv` by, the first entry of the result will be zero. The first entry corresponds to the constant monomial $1$. Hence, `D_pinv` corresponds to integration that sets the arbitrary constant of integration to zero. \n",
    "\n",
    "---\n",
    "\n",
    "There is a lot more that could be said about the Moore-Penrose pseudo-inverse and its applications, but you can read on your own or learn about this in later modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Singular Value Decomposition (SVD)\n",
    "\n",
    "The [Singular Value Decomposition (SVD)](https://en.wikipedia.org/wiki/Singular_value_decomposition) is one of the most significant and important results in linear algebra from the point of view of modern applied mathematics. \n",
    "See for example [here](https://twitter.com/WomenInStat/status/1285610321747611653).\n",
    "Unfortunately we will spend even less time on it than we did on the pseudo-inverse. The goal is just to make you aware of this important result.\n",
    "\n",
    "Any real matrix $A$ can be decomposed into the product of matrices: \n",
    "\n",
    "$$\n",
    "A =  U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "where $U$ and $V$ are orthogonal matrices, i.e. $U^T U = I$ and $V^T V = I$, and $\\Sigma$ is a diagonal matrix. The diagonal entries $\\sigma_i$ of $\\Sigma$ are know as the *singular values*.  The $\\sigma_i$ are all real and non-negative. All SVD algorithms will return these in order, from largest to smallest. The number of non-zero singular values is the rank of the matrix. Note, there is no assumption that the matrix $A$ is square. It is any real matrix (and can even be a complex matrix if $U$ and $V$ are generalised to complex matrices.)\n",
    "\n",
    "In just a few sentences, what is important here theoretically is that the SVD immediately gives the image, kernel, and rank of $A$. What is important for applications is that the dominant (largest) singular values can often capture extremely well the essential properties of $A$. As a result a large $M \\times N$ matrix can effectively be approximated by a much smaller one. A example would be image compression. A greyscale image of $M \\times N$ pixels is effectively a matrix with each matrix element corresponding to the greyscale value at the corresponding pixel. The SVD can be used to compress such a matrix as illustrated in Assignment 5. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
